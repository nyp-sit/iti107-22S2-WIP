{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-3/2.feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzhOTiHVcQW1"
   },
   "source": [
    "# Transfer learning - Feature Extraction\n",
    "\n",
    "In this exercise, we use transfer learning to improve our baseline model. We will use a pre-trained model (VGG19) as a feature extractor and use the extracted features to train a classifier for our emotion classification task.\n",
    "\n",
    "At the end of this exercise, you will be able to: \n",
    "- understand how to load a pretrained model with and without the classification layer  \n",
    "- extract features using the pre-trained model as feature extractor\n",
    "- train a classifier using the extracted features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE_WxnF6cQW2"
   },
   "source": [
    "Transfer learning involved using the \"knowledge\" learnt from another task (e.g. doing image classification on a large dataset such as ImageNet) and transfer that knowledge to a new and related task (e.g doing image classification on different types of objects than the original ones or for doing object detection). There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. Let's start with feature extraction approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhhojEoUcQW2"
   },
   "source": [
    "## Feature extraction\n",
    "\n",
    "In this approach, we only take the convolutional base of a pretrained model and use it to extract features from the images, and use the extracted features as input features to train a separate classifier. \n",
    "\n",
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/resources/swapping_fc_classifier.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-jVRqR4cQW4"
   },
   "source": [
    "### Using pre-trained Model as Feature Extractor\n",
    "\n",
    "We will be using VGG19 as our pretrained model (you can choose any other pretrained model, such as ResNet, etc). In the following code, we load the model VGG19 without including the classification layers (`include_top=False`). In the weights, we specify that we want to download the weights that was trained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3YwEl2QzcQW3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "path_to_zip = tf.keras.utils.get_file(origin=dataset_url, extract=True, cache_dir='.')\n",
    "dataset_folder = os.path.dirname(path_to_zip)\n",
    "dataset_folder = os.path.join(dataset_folder, 'flower_photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "image_size = (128,128)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_folder,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_folder,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOOmRfUJcQW6"
   },
   "outputs": [],
   "source": [
    "# Specify the intended image size we want\n",
    "image_size = (128, 128)\n",
    "base_model = keras.applications.efficientnet.EfficientNetB0(input_shape=image_size + (3,),\n",
    "                                      include_top=False,\n",
    "                                      weights='imagenet')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUCJI-mBcQW7"
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Examine the print out from `model.summary()`\n",
    "- What is the last layer in the pretrained model and what is the output shape? Do you have any Fully connected layers?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "The last layer is the MaxPooling2D layer. The output is a 512 feature maps of 4x4 size. There is no Fully connected (Dense) layers. The network is a convolutional base network.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1G3q88hcQW8"
   },
   "source": [
    "## Creating Datasets\n",
    "\n",
    "We will setup our training and validation dataset as we did in earlier exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3YNaBmEecQW8"
   },
   "outputs": [],
   "source": [
    "# dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz'\n",
    "# tf.keras.utils.get_file(origin=dataset_URL, extract=True, cache_dir='.')\n",
    "# dataset_folder = os.path.join('datasets', 'cats_and_dogs_subset')\n",
    "import os \n",
    "\n",
    "# dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz'\n",
    "# tf.keras.utils.get_file(origin=dataset_URL, extract=True, cache_dir='.')\n",
    "# dataset_folder = os.path.join('datasets', 'cats_and_dogs_subset')\n",
    "\n",
    "# dataset_URL = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/iti107/datasets/emotions_dataset_jpg.zip'\n",
    "# path_to_zip= keras.utils.get_file('emotions_dataset_jpg.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
    "# print(path_to_zip)\n",
    "# dataset_folder = os.path.dirname(path_to_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QWlVQdkcQW9"
   },
   "source": [
    "### Extracting features on the train set \n",
    "\n",
    "We will first define a function to perform feature extraction, given an image dataset. \n",
    "\n",
    "We can use `predict()` of the model to loop through all the train images (and also the validation images), or just pass the images directly to the keras model, e.g. `model(images)`. The output will be the features spit out by the convolutional base. We will then use these features as our training samples instead of the original images.\n",
    "\n",
    "However, before we pass the images through the convolutional base, it is IMPORTANT to pre-process the image using the model-specific preprocessing function. Many people *FORGOT* about this step. Different model expect the images to be of specific range of values (e.g. some models expect the pixel values to be between 0 and 1, some between -1 and 1) and specific channel ordering (e.g. VGGNet expects the channel to be BGR). So we need to make sure our images are pre-processed according to what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sD7rPV3zcQW9"
   },
   "outputs": [],
   "source": [
    "# retrieve the preprocess_input function of vgg16 for use later \n",
    "preprocess_input_fn = keras.applications.efficientnet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "W90bXhWHcQW-"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "def get_features_labels(dataset): \n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in dataset:   # each iteration yields a batch of images\n",
    "        # pre-process the features\n",
    "        preprocessed_images = preprocess_input_fn(images)\n",
    "        features = base_model(preprocessed_images)\n",
    "        \n",
    "        # append the batch of features to all_features and all_labels\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # concatenate the features from all the batches\n",
    "    all_features, all_labels = np.concatenate(all_features), np.concatenate(all_labels)\n",
    "    \n",
    "    return all_features, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijgudSr7cQW-"
   },
   "source": [
    "Now we will call the extract function on both training dataset and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Kb5ThObpcQW-"
   },
   "outputs": [],
   "source": [
    "# Extract features and labels for train set\n",
    "X_train, y_train = get_features_labels(train_ds)\n",
    "\n",
    "# Extract features and labels for validation set\n",
    "X_val, y_val = get_features_labels(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dNQvgD4EcQW-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2936, 4, 4, 1280)\n",
      "(734, 4, 4, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the features\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZrc_5cVcQW_"
   },
   "source": [
    "We will now save the features to local storage, as numpy arrays. We will load these features later on to be used for training our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8CirH4sWcQW_"
   },
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_val.npy\", y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKldlOhOcQW_"
   },
   "source": [
    "## Classification model\n",
    "\n",
    "Now we will build a new classification model that takes in the extracted features as input. Instead of the usual flatten layer, followed by dense layers, let us use a GAP layer, followed by Dense (with 512 units), a Dropout (with 50%) and another Dense that output the prediction. Compile your model using Adam with a learning rate of 0.001.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "1. What should be input shape to our model? \n",
    "2. What is the output shape of the Global Average Pooling (GAP) layer? \n",
    "3. How many units we need for output, and what should we use as activation function? \n",
    "\n",
    "Complete the code below. \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "1. The input shape should be (4, 4, 512) which is the output shape of our convolutional base\n",
    "2. The output shape of GAP is (512) since the maxpooling layer (the last layer) of the convolutional base has 512 feature maps (channels). \n",
    "3. We need only 1 output unit as we are doing binary classification (0 or 1) and we should use 'sigmoid' as the activation function for binary classification. \n",
    "\n",
    "Codes: \n",
    "\n",
    "```python\n",
    "inputs = keras.layers.Input(shape=X_train.shape[1:])\n",
    "x = keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "x = keras.layers.Dropout(rate=0.5)(x)\n",
    "x = keras.layers.Dense(units=512, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(rate=0.5)(x)\n",
    "outputs = keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
    "\n",
    "model_top.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "``` \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.layers.Input(shape=X_train.shape[1:])\n",
    "# x = keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "# x = keras.layers.Dropout(rate=0.5)(x)\n",
    "# x = keras.layers.Dense(units=512, activation=\"relu\")(x)\n",
    "# x = keras.layers.Dropout(rate=0.5)(x)\n",
    "# outputs = keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
    "\n",
    "# model_top.compile(loss=\"binary_crossentropy\", \n",
    "#                   optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "#                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "x = keras.layers.Dropout(rate=0.5)(x)\n",
    "x = keras.layers.Dense(units=128, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(rate=0.5)(x)\n",
    "outputs = keras.layers.Dense(units=5, activation=\"softmax\")(x)\n",
    "\n",
    "model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
    "\n",
    "model_top.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 1280)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "trMqDpjPcQW_"
   },
   "outputs": [],
   "source": [
    "# TODO: build your classification model here, try to use functional API to do so.\n",
    "\n",
    "# inputs = ??\n",
    "\n",
    "# ## any other layers\n",
    "\n",
    "# outputs = ??\n",
    "\n",
    "# model_top = keras.models.Model(inputs=[inputs], outputs=[outputs], name=\"top\")\n",
    "\n",
    "# model_top.compile(loss=??, \n",
    "#                   optimizer=??, \n",
    "#                   metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8oOdyJjYcQW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"top\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 4, 4, 1280)]      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 164,613\n",
      "Trainable params: 164,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVJWxOVgcQXA"
   },
   "source": [
    "Now we train our classifier we the extracted features (X_train) for 30 epochs. The training will be fast, as we only have very few parameters (around 200k) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jBd_NrImcQXA"
   },
   "outputs": [],
   "source": [
    "# we will now load the extracted features from the files we save to earlier\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "y_val = np.load('y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eczFKxhZm6ON"
   },
   "outputs": [],
   "source": [
    "# create the tensorboard callback\n",
    "import os\n",
    "import time\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "\n",
    "def get_run_logdir():    # use a new directory for each run\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tb_callback = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# create model checkpoint callback to save the best model checkpoint\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"best_checkpoint\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iKqD1T86cQXA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "184/184 [==============================] - 2s 6ms/step - loss: 0.7894 - accuracy: 0.7101 - val_loss: 0.3771 - val_accuracy: 0.8692\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4695 - accuracy: 0.8270 - val_loss: 0.3298 - val_accuracy: 0.8760\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.3708 - accuracy: 0.8641 - val_loss: 0.3145 - val_accuracy: 0.8747\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.3421 - accuracy: 0.8764 - val_loss: 0.2950 - val_accuracy: 0.8951\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8927 - val_loss: 0.2860 - val_accuracy: 0.8896\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2885 - accuracy: 0.8934 - val_loss: 0.2794 - val_accuracy: 0.8978\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - 1s 7ms/step - loss: 0.2537 - accuracy: 0.9063 - val_loss: 0.2888 - val_accuracy: 0.8965\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2412 - accuracy: 0.9142 - val_loss: 0.2905 - val_accuracy: 0.9005\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2349 - accuracy: 0.9114 - val_loss: 0.2962 - val_accuracy: 0.8856\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2171 - accuracy: 0.9183 - val_loss: 0.3029 - val_accuracy: 0.8896\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2199 - accuracy: 0.9203 - val_loss: 0.2946 - val_accuracy: 0.8924\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2034 - accuracy: 0.9261 - val_loss: 0.2955 - val_accuracy: 0.8924\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1992 - accuracy: 0.9281 - val_loss: 0.2932 - val_accuracy: 0.8937\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.2022 - accuracy: 0.9315 - val_loss: 0.2923 - val_accuracy: 0.8910\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1767 - accuracy: 0.9322 - val_loss: 0.2971 - val_accuracy: 0.8883\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1771 - accuracy: 0.9339 - val_loss: 0.3207 - val_accuracy: 0.8856\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1650 - accuracy: 0.9428 - val_loss: 0.3095 - val_accuracy: 0.8910\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1777 - accuracy: 0.9332 - val_loss: 0.3161 - val_accuracy: 0.8937\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1559 - accuracy: 0.9404 - val_loss: 0.3145 - val_accuracy: 0.8828\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1623 - accuracy: 0.9411 - val_loss: 0.3341 - val_accuracy: 0.8828\n",
      "Epoch 21/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1674 - accuracy: 0.9380 - val_loss: 0.3182 - val_accuracy: 0.8869\n",
      "Epoch 22/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1449 - accuracy: 0.9486 - val_loss: 0.3107 - val_accuracy: 0.8978\n",
      "Epoch 23/50\n",
      "184/184 [==============================] - 2s 10ms/step - loss: 0.1418 - accuracy: 0.9496 - val_loss: 0.3184 - val_accuracy: 0.8937\n",
      "Epoch 24/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1382 - accuracy: 0.9465 - val_loss: 0.3185 - val_accuracy: 0.9033\n",
      "Epoch 25/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1446 - accuracy: 0.9486 - val_loss: 0.3200 - val_accuracy: 0.8937\n",
      "Epoch 26/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1453 - accuracy: 0.9445 - val_loss: 0.3100 - val_accuracy: 0.8992\n",
      "Epoch 27/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1522 - accuracy: 0.9452 - val_loss: 0.3262 - val_accuracy: 0.8937\n",
      "Epoch 28/50\n",
      "184/184 [==============================] - 1s 7ms/step - loss: 0.1090 - accuracy: 0.9629 - val_loss: 0.3247 - val_accuracy: 0.8924\n",
      "Epoch 29/50\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1219 - accuracy: 0.9533 - val_loss: 0.3446 - val_accuracy: 0.8883\n",
      "Epoch 30/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1274 - accuracy: 0.9523 - val_loss: 0.3369 - val_accuracy: 0.8896\n",
      "Epoch 31/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1311 - accuracy: 0.9493 - val_loss: 0.3380 - val_accuracy: 0.8910\n",
      "Epoch 32/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1398 - accuracy: 0.9482 - val_loss: 0.3427 - val_accuracy: 0.8951\n",
      "Epoch 33/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1265 - accuracy: 0.9527 - val_loss: 0.3419 - val_accuracy: 0.8883\n",
      "Epoch 34/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1306 - accuracy: 0.9489 - val_loss: 0.3309 - val_accuracy: 0.8978\n",
      "Epoch 35/50\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1144 - accuracy: 0.9605 - val_loss: 0.3417 - val_accuracy: 0.9033\n",
      "Epoch 36/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1278 - accuracy: 0.9554 - val_loss: 0.3455 - val_accuracy: 0.8965\n",
      "Epoch 37/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1168 - accuracy: 0.9561 - val_loss: 0.3578 - val_accuracy: 0.8951\n",
      "Epoch 38/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1079 - accuracy: 0.9591 - val_loss: 0.3658 - val_accuracy: 0.8937\n",
      "Epoch 39/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1071 - accuracy: 0.9636 - val_loss: 0.3888 - val_accuracy: 0.8951\n",
      "Epoch 40/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1294 - accuracy: 0.9567 - val_loss: 0.3911 - val_accuracy: 0.8978\n",
      "Epoch 41/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1334 - accuracy: 0.9520 - val_loss: 0.3691 - val_accuracy: 0.8965\n",
      "Epoch 42/50\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1117 - accuracy: 0.9605 - val_loss: 0.3719 - val_accuracy: 0.9033\n",
      "Epoch 43/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1261 - accuracy: 0.9544 - val_loss: 0.3746 - val_accuracy: 0.8965\n",
      "Epoch 44/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1203 - accuracy: 0.9578 - val_loss: 0.3594 - val_accuracy: 0.9046\n",
      "Epoch 45/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1107 - accuracy: 0.9622 - val_loss: 0.3680 - val_accuracy: 0.8978\n",
      "Epoch 46/50\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0988 - accuracy: 0.9659 - val_loss: 0.3824 - val_accuracy: 0.9033\n",
      "Epoch 47/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1099 - accuracy: 0.9598 - val_loss: 0.3474 - val_accuracy: 0.9033\n",
      "Epoch 48/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1039 - accuracy: 0.9608 - val_loss: 0.3668 - val_accuracy: 0.8924\n",
      "Epoch 49/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.1035 - accuracy: 0.9608 - val_loss: 0.3634 - val_accuracy: 0.8937\n",
      "Epoch 50/50\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.0932 - accuracy: 0.9632 - val_loss: 0.3695 - val_accuracy: 0.8951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d65152310>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.fit(X_train, y_train, \n",
    "              epochs=50, \n",
    "              batch_size=16,\n",
    "              validation_data=(X_val, y_val), \n",
    "              callbacks=[tb_callback, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IwMQWsPfn8Wf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18604), started 6 days, 23:21:19 ago. (Use '!kill 18604' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-557c08d97146333e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-557c08d97146333e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOqWHsbXoN3K"
   },
   "source": [
    "Let's load the best-performing model checkpoints and use it to compute classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DPP1F8f4cQXA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3594 - accuracy: 0.9046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3594142496585846, 0.9046321511268616]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.load_weights('best_checkpoint')\n",
    "model_top.evaluate(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bs-21XRcQXB"
   },
   "source": [
    "You should see an good improvement in the model (should be around 30%). The model also takes much less time to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqrdAJdwcQXB"
   },
   "source": [
    "## Prepare the model for deployment\n",
    "\n",
    "We cannot use our `model_top` directly for image classification, as it take extracted features as input, not images. We need to stick back our convolutional base that can take in images directly. This is what we are going to do below. It is also important to include the model-specific pre-processing function as one of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IIU3MBx7cQXB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb0 (Functional)  (None, 4, 4, 1280)        4049571   \n",
      "_________________________________________________________________\n",
      "top (Functional)             (None, 5)                 164613    \n",
      "=================================================================\n",
      "Total params: 4,214,184\n",
      "Trainable params: 164,613\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# specify the input layer with appropriate image shape\n",
    "inputs = keras.layers.Input(shape=image_size+(3,))\n",
    "\n",
    "# import to include model-specific preprocess function\n",
    "x = preprocess_input_fn(inputs)\n",
    "\n",
    "x = base_model(x)\n",
    "outputs = model_top(x)\n",
    "\n",
    "model_full = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model_full.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yGf_nY58cQXC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model_full.save(\"full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ciRBIOgqtQ5"
   },
   "source": [
    "Let's make sure our full model works on the validation dataset (which are images) and gives the same accuracy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DaBScJj3cQXC"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1330 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1269 test_step\n        self.compiled_loss(\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:245 sigmoid_cross_entropy_with_logits_v2\n        return sigmoid_cross_entropy_with_logits(\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:132 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 5) vs (None, 1))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1501\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1500\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1501\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1503\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:759\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_internal_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3066\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3066\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3301\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3303\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3306\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3307\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1007\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1012\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 668\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:994\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    993\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    995\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1330 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py:1269 test_step\n        self.compiled_loss(\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\keras\\backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:245 sigmoid_cross_entropy_with_logits_v2\n        return sigmoid_cross_entropy_with_logits(\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\markk\\miniconda3\\envs\\dlenv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:132 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 5) vs (None, 1))\n"
     ]
    }
   ],
   "source": [
    "model_full.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKxz3wUIcQXD"
   },
   "source": [
    "## Extra exercises\n",
    "\n",
    "Try another pre-trained model such as MobileNetV2 or EfficientNetB1 and see if the extracted features give you better classification result. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "2.feature_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
